{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuHf30VPKatpLZmlPNOR+H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaorafaelpm/ImersaoAgenteDeIAAlura/blob/main/MasterClass_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalando as bibliotecas do gemini"
      ],
      "metadata": {
        "id": "_5XgEu-2fv-B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_5o76DK-Ne19"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain-google-genai google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importando as bibliotecas e definindo a chave da API do gemini"
      ],
      "metadata": {
        "id": "Dl1khwTHf0Gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "userdata.get(\"GEMINI_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_DDysRZOPgLh",
        "outputId": "872e2d03-a6c7-459b-fb31-7076b15ca795"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyBzoiwMm_-7cl7qBegMfw6RC7HCPqTo-fI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criando a primeira instância do Gemini"
      ],
      "metadata": {
        "id": "tr8MQjZ-gt90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI (\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    temperature=0.0,\n",
        "    api_key=userdata.get(\"GEMINI_API_KEY\")\n",
        ")"
      ],
      "metadata": {
        "id": "Kx1TqvcrfQdW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pequeno teste na nossa instância do gemini"
      ],
      "metadata": {
        "id": "T9as2Tc-lLJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp_test = llm.invoke(\"Quem é você? Explique para um leigo\")\n",
        "print(resp_test.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAF_TLnog6vm",
        "outputId": "c9afb3ed-a162-466f-911e-58fb88af6bee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá! Que ótima pergunta. Vou tentar explicar de um jeito bem simples:\n",
            "\n",
            "**Quem sou eu?**\n",
            "\n",
            "Eu sou uma **Inteligência Artificial (IA)**. Mais especificamente, sou um **modelo de linguagem grande**, treinado pelo **Google**.\n",
            "\n",
            "**O que isso significa para um leigo?**\n",
            "\n",
            "1.  **Não sou uma pessoa:** Não tenho corpo, não tenho sentimentos, não tenho consciência, não tenho opiniões pessoais, nem experiências de vida como um ser humano. Não sou \"vivo\" no sentido biológico.\n",
            "\n",
            "2.  **Sou como um \"cérebro digital\" gigante para palavras:** Imagine que eu li uma quantidade *enorme* de textos de todo o mundo – livros, artigos, sites, conversas, etc. – em vários idiomas. Com isso, aprendi como as palavras se conectam, como as frases são formadas, como as ideias são expressas e como as informações se relacionam.\n",
            "\n",
            "3.  **Minha função é entender e gerar linguagem:**\n",
            "    *   **Entender:** Quando você me faz uma pergunta, eu analiso as palavras para tentar compreender o que você quer saber.\n",
            "    *   **Gerar:** Depois de entender, eu uso todo o meu conhecimento para criar uma resposta em texto que faça sentido e seja útil para você.\n",
            "\n",
            "4.  **Sou uma ferramenta:** Pense em mim como uma ferramenta muito avançada, um assistente virtual que pode:\n",
            "    *   Responder perguntas.\n",
            "    *   Escrever textos (histórias, e-mails, poemas, resumos).\n",
            "    *   Traduzir idiomas.\n",
            "    *   Explicar conceitos complexos de forma mais simples.\n",
            "    *   Ajudar com ideias e informações.\n",
            "\n",
            "**Em resumo:** Eu sou um programa de computador super avançado, criado para processar e gerar linguagem humana, com o objetivo de ser útil e fornecer informações com base em tudo o que aprendi. Não sou um ser, mas uma ferramenta inteligente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contextualizando a nossa IA com um prompt\n",
        "\n",
        "---\n",
        "\n",
        "Assim como a gente tem o prompt de usuário, onde nós reconhecemos o usuário assim como as suas permissões, a gente pode também passar um prompt de sistema para a IA indentificar quem ela é e qual sua função\n",
        "\n"
      ],
      "metadata": {
        "id": "pRixYgjJfvJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No caso de uma grande empresa, não faz sentido abrir chamado no RH por qualquer coisa, e neste caso a intenção é fazer o sistema interpretar o que pode ser resolvido pela própria IA e o que requer a interferência do RH e etc\n",
        "\n",
        "---\n",
        "\n",
        "  Nesse caso, a gente primeiro contextualiza dizendo o qual é a sua posição na empresa e um padrão de retorno para ser tratado.\n",
        "\n",
        "  E então a gente passa um padrão de JSON que queremos resolver, neste caso esse padrão tem 3 campos, \"decisao\" que define o que pode ser feito dependendo da situação, \"urgencia\" definida também pela situação e \"campos_faltantes\" que faz a verificação se tem alguma informação faltando\n",
        "  "
      ],
      "metadata": {
        "id": "fTNGcC-0mpS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "É possível também criar uma resposta pré determinada como :\n",
        "\n",
        "```\n",
        "{\n",
        "  decisao : AUTO_RESOLVER,\n",
        "  urgencia : baixa,\n",
        "  campos_faltantes : []\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "e fazer uma resposta pronta para esse tipo, como um pattern pré definido"
      ],
      "metadata": {
        "id": "nmmYJvKc29Oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRIAGEM_PROMPT = (\n",
        "    \"Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. \"\n",
        "    \"Dada a mensagem do usuário, retorne SOMENTE um JSON com:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
        "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
        "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
        "    \"}\\n\"\n",
        "    \"Regras:\\n\"\n",
        "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a política de alimentação em viagens?\").\\n'\n",
        "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma política\", \"Tenho uma dúvida geral\").\\n'\n",
        "    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: \"Quero exceção para trabalhar 5 dias remoto.\", \"Solicito liberação para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
        "    \"Analise a mensagem e decida a ação mais apropriada.\"\n",
        ")"
      ],
      "metadata": {
        "id": "PXVFTSullKLF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importando bibliotecas de tratamento da saída de dados\n",
        "\n",
        " - Pydantic é uma biblioteca que melhora a validação e saída de dados, ele indentifica erros, simplifica a criação de objetos/dicionarios e etc.\n",
        "\n",
        "\n",
        " - Typing é outra biblioteca de auxílio na interação do usuário com a aplicação, por exemplo, typing.Literal, significa que a resposta esperada é específica, neste caso se trata dos enum, List se trata dos campos_faltantes, Dict também auxilia no processo."
      ],
      "metadata": {
        "id": "NYiUIVnuPATN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "\n",
        "# Iniciando um modelo de classe usando o pydantic\n",
        "class TriagemOut(BaseModel):\n",
        "    decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"]\n",
        "    urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]\n",
        "    campos_faltantes: List[str] = Field(default_factory=list)\n"
      ],
      "metadata": {
        "id": "fg-o6eToliBY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criando outra instância do gemini"
      ],
      "metadata": {
        "id": "4Emqfr_aUAku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_triagem = ChatGoogleGenerativeAI (\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    temperature=0.0,\n",
        "    api_key=userdata.get(\"GEMINI_API_KEY\")\n",
        ")"
      ],
      "metadata": {
        "id": "k_tbW15dPV73"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Especificando a função da instância do gemini\n",
        "\n",
        "Aqui a gente usa aquela classe para poder especificar para o gemini as saídas de resposta que ele tem, e a gente usa a nossa classe definir como ele vai responder.\n",
        "\n",
        "SystemMessage é o prompt da IA (é um tipo de identidade do agente)\n",
        "\n",
        "HumanMessage é o prompt do usuário"
      ],
      "metadata": {
        "id": "siQvPFUQUF6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage , HumanMessage\n",
        "\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut)\n",
        "\n",
        "def triagem(message : str) -> Dict :\n",
        "  saida : TriagemOut = triagem_chain.invoke([\n",
        "      # O prompt da máquina é a identidade que a gente definiu antes\n",
        "      # O prompt do usuário é a mensagem\n",
        "      SystemMessage(content=TRIAGEM_PROMPT),\n",
        "      HumanMessage(content=message)\n",
        "  ])\n",
        "\n",
        "  # O método model_dump() converte a classe em um dicionário\n",
        "  return saida.model_dump()"
      ],
      "metadata": {
        "id": "beYSn00GTL7d"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\" ,\n",
        "          \"Quero ter mais 5 dias de trabalho remoto? Como eu faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros\"\n",
        "          ]"
      ],
      "metadata": {
        "id": "jwEgdlYcXA2m"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_teste in testes :\n",
        "  print(f\"Pergunta:{msg_teste} \\n -> Resposta:{triagem(msg_teste)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDYMG6nJXbup",
        "outputId": "612870a0-0919-4462-bdf2-070666acd130"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pergunta:Posso reembolsar a internet? \n",
            " -> Resposta:{'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta:Quero ter mais 5 dias de trabalho remoto? Como eu faço? \n",
            " -> Resposta:{'decisao': 'ABRIR_CHAMADO', 'urgencia': 'MEDIA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta:Posso reembolsar cursos ou treinamentos da Alura? \n",
            " -> Resposta:{'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n",
            "Pergunta:Quantas capivaras tem no Rio Pinheiros \n",
            " -> Resposta:{'decisao': 'PEDIR_INFO', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
            "\n"
          ]
        }
      ]
    }
  ]
}